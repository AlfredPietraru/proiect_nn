{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cdac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from voc import get_dataloader\n",
    "from main_utils import set_seed\n",
    "from model_factory import get_model\n",
    "from ema import build_ema, EMA\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31451bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIZE = (256, 256)\n",
    "import os\n",
    "SIZE = (160, 160)\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "BATCH_SIZE = 4\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def scale_to_01(image, **kwargs):\n",
    "    return image.astype('float32') / 255.0\n",
    "\n",
    "train_labeled_transforms = A.Compose([\n",
    "    A.Resize(SIZE[0], SIZE[1]),         \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Lambda(image=scale_to_01), \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "train_unlabeled_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(SIZE[0], SIZE[1]),\n",
    "            A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.8),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), sigma_limit=(0.1, 2.0), p=0.5),\n",
    "            A.CoarseDropout(num_holes_range=(3, 3), hole_height_range=(0.05, 0.1),\n",
    "                             hole_width_range=(0.05, 0.1), p=0.5),\n",
    "            A.Lambda(image=scale_to_01), \n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='pascal_voc')\n",
    "    )\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(SIZE[0], SIZE[1]),\n",
    "    A.Lambda(image=scale_to_01), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "\n",
    "\n",
    "dt_train_labeled = get_dataloader(\"trainval\", \"2007\", BATCH_SIZE, transform=train_labeled_transforms)\n",
    "dt_train_unlabeled_weakaug = get_dataloader(\"trainval\", \"2012\", BATCH_SIZE, transform=train_labeled_transforms) \n",
    "dt_train_unlabeled_strongaug = get_dataloader(\"trainval\", \"2012\", BATCH_SIZE, transform=train_unlabeled_transforms)\n",
    "dt_test = get_dataloader(\"test\", \"2007\", BATCH_SIZE, transform=test_transforms, shuffle=False)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6da2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "METRIC_KEYS = [\"loss_classifier\", \"loss_box_reg\", \"loss_objectness\", \"loss_rpn_box_reg\", \"total\"]\n",
    "\n",
    "def plot_losses(history):\n",
    "    epochs = range(1, len(history[\"total\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for comp in METRIC_KEYS:\n",
    "        plt.plot(epochs, history[comp], label=f\"Train {comp}\", linewidth=2)\n",
    "    plt.title(f\"Train results over epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331ecaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_checkpoint(checkpoint_path, optimizer=None, device='cuda'):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = get_model(device=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model weights loaded from {checkpoint_path}\")\n",
    "\n",
    "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Optimizer state loaded from {checkpoint_path}\")\n",
    "\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    print(f\"Resuming from epoch {epoch}\")\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "def train(model, optimizer, dt_train_labeled, device):\n",
    "    model.train()\n",
    "    train_batches = 0\n",
    "    history = {key : 0 for key in METRIC_KEYS}\n",
    "\n",
    "    for images, targets in tqdm(dt_train_labeled, desc=\"Training\"):\n",
    "        # if train_batches == 5: break\n",
    "        for target in targets:\n",
    "            target[\"boxes\"] = target[\"boxes\"].to(device)\n",
    "            target[\"labels\"] = target[\"labels\"].to(device)\n",
    "        images = images.to(device)\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for k, v in loss_dict.items():\n",
    "            history[k] += v.item()\n",
    "\n",
    "        history[\"total\"] += loss.item()\n",
    "        train_batches += 1\n",
    "    for key in history:\n",
    "        history[key] = history[key] / train_batches\n",
    "    return history\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at {path}\")\n",
    "\n",
    "\n",
    "def pipeline(epochs, dt_train_labeled, device, checkpoint_every):\n",
    "\n",
    "    model = get_model(device=device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    history = {key : [] for key in METRIC_KEYS}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n==================== Epoch {epoch+1}/{epochs} ====================\\n\")\n",
    "        train_history = train(model, optimizer, dt_train_labeled, device)\n",
    "        lr_scheduler.step(train_history[\"total\"])\n",
    "        for key, val in train_history.items():\n",
    "            history[key].append(val)\n",
    "        plot_losses(history)\n",
    "        if (epoch + 1) % checkpoint_every == 0 or (epoch + 1) == epochs:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            save_checkpoint(model, optimizer, epoch + 1, checkpoint_path)\n",
    "\n",
    "# pipeline(30, dt_train_labeled, device, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dt_train_unlabeled_weakaug))\n",
    "from torchvision.ops import batched_nms\n",
    "NMS_IOU = 0.5\n",
    "def generate_pseudo_labels(model : torch.nn.Module, images : torch.Tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images, None)\n",
    "        for output in outputs:\n",
    "            boxes  = output[\"boxes\"]\n",
    "            labels = output[\"labels\"]\n",
    "            scores = output[\"scores\"]\n",
    "\n",
    "            keep_nms = batched_nms(\n",
    "                boxes, scores, labels,\n",
    "                iou_threshold=NMS_IOU\n",
    "            )\n",
    "            boxes  = boxes[keep_nms]\n",
    "            labels = labels[keep_nms]\n",
    "            scores = scores[keep_nms]\n",
    "\n",
    "            boxes_to_keep = scores > CONFIDENCE_THRESHOLD        \n",
    "            boxes  = boxes[boxes_to_keep]\n",
    "            labels = labels[boxes_to_keep]\n",
    "            scores = scores[boxes_to_keep]\n",
    "\n",
    "            output[\"boxes\"]  = boxes\n",
    "            output[\"labels\"] = labels\n",
    "            output[\"scores\"] = scores\n",
    "        return outputs       \n",
    "    \n",
    "# model, optimizer, epoch = load_checkpoint(checkpoint_path=checkpoint_path, optimizer=None, device=device)\n",
    "# generate_pseudo_labels(model, images, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406f6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from checkpoints/checkpoint_epoch_42.pth\n",
      "Resuming from epoch 42\n",
      "\n",
      "==================== Epoch 1/10 ====================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss_box_reg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m             history[key].append(val)\n\u001b[32m     43\u001b[39m checkpoint_path=\u001b[33m\"\u001b[39m\u001b[33mcheckpoints/checkpoint_epoch_42.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mrun_semi_supervised_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_train_unlabeled_weakaug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_train_unlabeled_strongaug\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mrun_semi_supervised_pipeline\u001b[39m\u001b[34m(checkpoint_path, epochs, dt_weak, dt_strong)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m==================== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ====================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     train_history = \u001b[43mtrain_self_supervised_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_weak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_strong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     lr_scheduler.step(train_history[\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m train_history.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_self_supervised_one_epoch\u001b[39m\u001b[34m(teacher, student, optimizer, dt_weak, dt_strong)\u001b[39m\n\u001b[32m     18\u001b[39m teacher.update(student)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict.items():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m += v.item()\n\u001b[32m     22\u001b[39m history[\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m] += loss.item()\n\u001b[32m     23\u001b[39m train_batches += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'loss_box_reg'"
     ]
    }
   ],
   "source": [
    "def train_self_supervised_one_epoch(teacher : EMA, student, optimizer, dt_weak, dt_strong):\n",
    "    student.train()\n",
    "    train_batches = 0\n",
    "    history = {key : 0 for key in [\"loss_classifier\", \"loss_objectness\", \"total\"]}\n",
    "    for (img_weak, _), (img_strong, _) in zip(dt_weak, dt_strong):\n",
    "        # SHOULD REPLACE THE TRANSFORMATION OF HORIZONTAL FLIP WITH SOMETHING PHOTOMETRIC\n",
    "        weak_targets = generate_pseudo_labels(teacher.ema, img_weak, device)\n",
    "        \n",
    "        for target in weak_targets:\n",
    "            target[\"boxes\"] = target[\"boxes\"].to(device)\n",
    "            target[\"labels\"] = target[\"labels\"].to(device)\n",
    "        img_strong = img_strong.to(device)\n",
    "        loss_dict = student(img_strong, weak_targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        teacher.update(student)\n",
    "        for k, v in loss_dict.items():\n",
    "            history[k] += v.item()\n",
    "\n",
    "        history[\"total\"] += loss.item()\n",
    "        train_batches += 1\n",
    "    for key in history:\n",
    "        history[key] = history[key] / train_batches\n",
    "    return history\n",
    "\n",
    "\n",
    "def run_semi_supervised_pipeline(checkpoint_path, epochs, dt_weak, dt_strong):\n",
    "    student, _, _ = load_checkpoint(checkpoint_path=checkpoint_path, optimizer=None, device=device)\n",
    "    teacher = build_ema(student)\n",
    "    optimizer = torch.optim.SGD(student.parameters(), lr=1e-2, momentum=0.9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    history = {key : [] for key in METRIC_KEYS}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n==================== Epoch {epoch+1}/{epochs} ====================\\n\")\n",
    "        train_history = train_self_supervised_one_epoch(teacher, student, optimizer, dt_weak, dt_strong)\n",
    "        lr_scheduler.step(train_history[\"total\"])\n",
    "        for key, val in train_history.items():\n",
    "            history[key].append(val)\n",
    "\n",
    "checkpoint_path=\"checkpoints/checkpoint_epoch_42.pth\"\n",
    "run_semi_supervised_pipeline(checkpoint_path, 10, dt_train_unlabeled_weakaug, dt_train_unlabeled_strongaug)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
